{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB_me_nSEaZK",
        "outputId": "d7b76a50-12a6-489e-a525-c15a57ca0c79"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset\n",
        "Note: we will need to turn the label values into -1 and 1."
      ],
      "metadata": {
        "id": "auWjo33EGHTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv(\"/content/gdrive/My Drive/CS 5350/Homework 3/bank-note/train.csv\", names=['variance', 'skewness', 'curtosis', 'entropy', 'label'])\n",
        "print(train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw3M5MMpEf4H",
        "outputId": "0166664b-7478-4c08-d46f-d9810234323f"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   variance  skewness  curtosis  entropy  label\n",
            "0  3.848100  10.15390  -3.85610 -4.22280      0\n",
            "1  4.004700   0.45937   1.36210  1.61810      0\n",
            "2 -0.048008  -1.60370   8.47560  0.75558      0\n",
            "3 -1.266700   2.81830  -2.42600 -1.88620      1\n",
            "4  2.203400   5.99470   0.53009  0.84998      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['label'] = train['label'].apply(lambda x: 1 if x == 1 else -1)\n",
        "print(train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCG7NEKYGRxO",
        "outputId": "b1970432-9d17-4885-a478-e5ed525cd25f"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   variance  skewness  curtosis  entropy  label\n",
            "0  3.848100  10.15390  -3.85610 -4.22280     -1\n",
            "1  4.004700   0.45937   1.36210  1.61810     -1\n",
            "2 -0.048008  -1.60370   8.47560  0.75558     -1\n",
            "3 -1.266700   2.81830  -2.42600 -1.88620      1\n",
            "4  2.203400   5.99470   0.53009  0.84998     -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\"/content/gdrive/My Drive/CS 5350/Homework 3/bank-note/test.csv\", names=['variance', 'skewness', 'curtosis', 'entropy', 'label'])\n",
        "print(test.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZR-WurHZGl5P",
        "outputId": "d5c43d20-883f-420a-b5d0-2fb97e94b152"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   variance  skewness  curtosis   entropy  label\n",
            "0   3.83840    6.1851  -2.04390 -0.033204      0\n",
            "1   2.85210    9.1710  -3.64610 -1.204700      0\n",
            "2   5.24180   10.5388  -4.11740 -4.279700      0\n",
            "3  -2.26230   12.1177   0.28846 -7.758100      0\n",
            "4   0.55298   -3.4619   1.70480  1.100800      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test['label'] = test['label'].apply(lambda x: 1 if x == 1 else -1)\n",
        "print(test.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_mPCyA8GuUb",
        "outputId": "1c69e8d9-004f-473c-9667-78702b6ab353"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   variance  skewness  curtosis   entropy  label\n",
            "0   3.83840    6.1851  -2.04390 -0.033204     -1\n",
            "1   2.85210    9.1710  -3.64610 -1.204700     -1\n",
            "2   5.24180   10.5388  -4.11740 -4.279700     -1\n",
            "3  -2.26230   12.1177   0.28846 -7.758100     -1\n",
            "4   0.55298   -3.4619   1.70480  1.100800      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our Class:"
      ],
      "metadata": {
        "id": "aNeKlxeCEW5T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "IA1D9TLD6G2O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "class Perceptron:\n",
        "  def standard_perceptron(self, X: np.ndarray, Y: np.ndarray, rate: int=1, epoch: int=1):\n",
        "    \"\"\"\n",
        "    This only works when the output is binary in the form of -1 or 1.\n",
        "    \"\"\"\n",
        "    weights: np.ndarray = np.zeros(len(X[0]))\n",
        "    copy_X = X[:]\n",
        "    copy_Y = Y[:]\n",
        "    for _ in range(epoch):\n",
        "      copy_X, copy_Y = shuffle(copy_X, copy_Y)\n",
        "      for row, y in zip(copy_X, copy_Y):\n",
        "        y_pred = np.sign(np.dot(weights, row)).astype(np.int64)\n",
        "        if y != y_pred:\n",
        "          weights = weights + rate * (y * row)\n",
        "    return weights\n",
        "\n",
        "  def voted_perceptron(self, X: np.ndarray, Y: np.ndarray, rate: int=1, epoch: int=1):\n",
        "    \"\"\"\n",
        "    This only works when the output is binary in the form of -1 or 1.\n",
        "    returns weights in the form of a list of tuples (weights_i, c_i)\n",
        "    \"\"\"\n",
        "    weights_array: list[tuple] = []\n",
        "    weights: np.ndarray = np.zeros(len(X[0]))\n",
        "    c: int = 0\n",
        "\n",
        "    copy_X = X[:]\n",
        "    copy_Y = Y[:]\n",
        "\n",
        "    for _ in range(epoch):\n",
        "      copy_X, copy_Y = shuffle(copy_X, copy_Y)\n",
        "      for row, y in zip(copy_X, copy_Y):\n",
        "        y_pred = np.sign(np.dot(weights, row)).astype(np.int64)\n",
        "        if y != y_pred:\n",
        "          weights_array.append((weights, c))\n",
        "          weights = weights + rate * (y * row)\n",
        "          c = 1\n",
        "        else:\n",
        "          c += 1\n",
        "    return weights_array\n",
        "\n",
        "  def average_perceptron(self, X: np.ndarray, Y: np.ndarray, rate: int=1, epoch: int=1):\n",
        "    \"\"\"\n",
        "    This only works when the output is binary in the form of -1 or 1.\n",
        "    \"\"\"\n",
        "    weights: np.ndarray = np.zeros(len(X[0]))\n",
        "    a: np.ndarray = np.zeros(len(X[0]))\n",
        "    copy_X = X[:]\n",
        "    copy_Y = Y[:]\n",
        "    for _ in range(epoch):\n",
        "      copy_X, copy_Y = shuffle(copy_X, copy_Y)\n",
        "      for row, y in zip(copy_X, copy_Y):\n",
        "        y_pred = np.sign(np.dot(weights, row)).astype(np.int64)\n",
        "        if y != y_pred:\n",
        "          weights = weights + rate * (y * row)\n",
        "        a += weights\n",
        "    return a\n",
        "\n",
        "  @staticmethod\n",
        "  def voted_perceptron_predict(datapoint: np.ndarray, weights: list):\n",
        "    predictions: float = 0\n",
        "    for i in range(len(weights)):\n",
        "      predictions += np.dot(weights[i][0], datapoint) * weights[i][1]\n",
        "    return np.sign(predictions).astype(np.int32)\n",
        "\n",
        "  @staticmethod\n",
        "  def bias_trick(X: np.ndarray):\n",
        "    \"\"\"\n",
        "    Used to add 1 to the input array\n",
        "    \"\"\"\n",
        "    output: list[np.ndarray] = []\n",
        "    for i in range(len(X)):\n",
        "      output.append(np.append(X[i], 1))\n",
        "    return np.array(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick way to check for if bias_trick works."
      ],
      "metadata": {
        "id": "s5SAOD6FD0fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array_ex: np.ndarray = np.array([np.array([1, 3, 3]), np.array([2, 3, 3]), np.array([3, 3, 4]), np.array([4, 3, 1])])\n",
        "print(Perceptron.bias_trick(array_ex))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LjyXXYEBZ_z",
        "outputId": "54e2482f-c1aa-4f3b-8ece-c6a29f1b2f59"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 3 3 1]\n",
            " [2 3 3 1]\n",
            " [3 3 4 1]\n",
            " [4 3 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will look at the accuracy of the standard perceptron."
      ],
      "metadata": {
        "id": "9Un1jDMhAjUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(Perceptron().standard_perceptron(X = Perceptron.bias_trick(train.drop('label', axis=1).values), Y = train['label'].values, epoch=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMOEsqXpAI_v",
        "outputId": "96be59d4-953d-4548-ba21-4f30b2e11e1e"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-58.8350927 -30.82818   -39.7006956  -5.670603   58.       ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets make test predictions."
      ],
      "metadata": {
        "id": "sqFJpkbMNpt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_standard_perceptron(y_test: np.ndarray, x_test: np.ndarray, weights: np.ndarray):\n",
        "  num_observations: int = len(y_test)\n",
        "  correct: int = 0\n",
        "  for i in range(num_observations):\n",
        "    prediction = np.sign(np.dot(weights, x_test[i]))\n",
        "    if prediction == y_test[i]:\n",
        "      correct += 1\n",
        "  return correct / num_observations"
      ],
      "metadata": {
        "id": "Fvi8jdm5Ntje"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = Perceptron().standard_perceptron(X = Perceptron.bias_trick(train.drop('label', axis=1).values), Y = train['label'].values, epoch=10)\n",
        "print(\"standard weights: \", weights)\n",
        "print(\"train accuracy: \", accuracy_standard_perceptron(train['label'].values, Perceptron.bias_trick(train.drop('label', axis=1).values), weights))\n",
        "print(\"test accuracy: \", accuracy_standard_perceptron(test['label'].values, Perceptron.bias_trick(test.drop('label', axis=1).values), weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA9mVM7rQcbf",
        "outputId": "17c070eb-fbd6-4e69-b9aa-d746ea796c80"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "standard weights:  [-52.6733   -38.93741  -45.327916  -9.518022  57.      ]\n",
            "train accuracy:  0.9908256880733946\n",
            "test accuracy:  0.984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick check of if everything is as expected."
      ],
      "metadata": {
        "id": "Ct6FuuU_saL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = Perceptron().voted_perceptron(X = Perceptron.bias_trick(train.drop('label', axis=1).values), Y = train['label'].values, epoch=10)\n",
        "print(len(weights))"
      ],
      "metadata": {
        "id": "lSQ-zCOgQjjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74181a70-2fda-4338-8f8e-8a67c639e7b8"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets test predictions."
      ],
      "metadata": {
        "id": "2ryw75HUseZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = Perceptron.voted_perceptron_predict(Perceptron.bias_trick(train.drop('label', axis=1).values)[0], weights)\n",
        "print(prediction, train['label'].values[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS3GKPEvshD5",
        "outputId": "5dcb8d06-021c-4d25-9879-ee80ceb8dc7b"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1 -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets test accuracy."
      ],
      "metadata": {
        "id": "N_pV-QHAuBWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_voted_perceptron(y_test: np.ndarray, x_test: np.ndarray, weights: list[tuple]):\n",
        "  num_observations: int = len(y_test)\n",
        "  correct: int = 0\n",
        "  for i in range(num_observations):\n",
        "    prediction = Perceptron.voted_perceptron_predict(x_test[i], weights)\n",
        "    if prediction == y_test[i]:\n",
        "      correct += 1\n",
        "  return correct / num_observations"
      ],
      "metadata": {
        "id": "aXrQjVsltRsf"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to print the count of the weights along with the weights for the voted perceptron."
      ],
      "metadata": {
        "id": "XyoQHMoohVau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = Perceptron().voted_perceptron(X = Perceptron.bias_trick(train.drop('label', axis=1).values), Y = train['label'].values, epoch=10)\n",
        "print(\"train accuracy: \", accuracy_voted_perceptron(train['label'].values, Perceptron.bias_trick(train.drop('label', axis=1).values), weights))\n",
        "print(\"test accuracy: \", accuracy_voted_perceptron(test['label'].values, Perceptron.bias_trick(test.drop('label', axis=1).values), weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcUXleOctxFV",
        "outputId": "e87949a1-78b1-4b1e-8484-2038e414d178"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train accuracy:  0.9885321100917431\n",
            "test accuracy:  0.986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for weight, count in weights:\n",
        "  print(count, weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk2tctvjh5KJ",
        "outputId": "68b74236-791f-4478-91b5-ce226c8c4731"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [0. 0. 0. 0. 0.]\n",
            "1 [-2.9719  -6.8369   0.2702  -0.71291 -1.     ]\n",
            "4 [-5.8865  -2.7832  -0.18679 -4.74561  0.     ]\n",
            "2 [-4.8313  -1.5975  -2.82789 -4.63528  1.     ]\n",
            "1 [ -3.7912  -10.9962   -3.68787   0.69832   0.     ]\n",
            "6 [-7.8592  -8.0599  -5.88707  0.19748 -1.     ]\n",
            "6 [-7.2587   -7.06045  -8.09967   0.294879  0.      ]\n",
            "2 [-10.8814    -3.06465   -8.45812   -3.609821   1.      ]\n",
            "12 [-11.65601   -4.94145   -6.05582   -2.477921   2.      ]\n",
            "2 [-10.20591   -1.33475  -10.11152   -4.074521   3.      ]\n",
            "37 [-11.95381   -7.15775   -4.24162   -2.862521   4.      ]\n",
            "9 [-10.16631   -2.37775   -9.37782   -6.098721   5.      ]\n",
            "4 [-9.42203  -6.15005  -7.76472  -4.523321  6.      ]\n",
            "5 [-10.08211   -9.37605   -3.95892   -3.339721   7.      ]\n",
            "14 [-11.53001   -4.49665  -12.30172   -1.231121   6.      ]\n",
            "21 [-13.74831   -5.75065   -9.30312   -0.867341   7.      ]\n",
            "2 [-18.30141  -18.33605    6.13858   -2.365641   8.      ]\n",
            "16 [-19.88131  -13.62845   -1.78002   -0.816941   7.      ]\n",
            "10 [-19.67915  -11.71025   -5.06282   -1.434621   8.      ]\n",
            "38 [-19.13979   -7.81585   -9.87942   -5.776421   9.      ]\n",
            "5 [-17.83979  -18.08365   -6.92642    0.087379   8.      ]\n",
            "16 [-24.04009    -9.40305    -6.9172856  -3.615621    9.       ]\n",
            "2 [-23.39537    -4.79685   -15.2642856  -0.905721    8.       ]\n",
            "2 [-24.78407    -9.67415    -8.7868856  -0.563931    9.       ]\n",
            "10 [-25.11331    -5.21895   -13.3586856   0.424869    8.       ]\n",
            "2 [-28.50171   -13.43395    -3.0271856   1.406739    9.       ]\n",
            "11 [-29.02121   -10.17065    -6.1166856   2.391639    8.       ]\n",
            "71 [-28.46182   -10.48105    -5.9336156   2.838169    9.       ]\n",
            "10 [-27.45012    -9.57885    -8.2842156   3.265309   10.       ]\n",
            "2 [-26.555      -4.80505   -13.1273156  -2.325591   11.       ]\n",
            "1 [-24.1633     -0.24855   -18.1161156  -5.224291   12.       ]\n",
            "5 [-23.78693    -1.07213   -17.3306856  -4.479051   13.       ]\n",
            "49 [-26.08563    -6.29913   -11.7006856  -3.561831   14.       ]\n",
            "8 [-23.63833   -18.92383   -12.4364156   4.099369   13.       ]\n",
            "24 [-29.52013   -11.26543   -11.8806156   1.183869   14.       ]\n",
            "4 [-28.61606    -7.89463   -16.3793156  -2.512631   15.       ]\n",
            "5 [-28.40175    -8.58992   -15.5022056  -2.216101   16.       ]\n",
            "32 [-28.59256   -17.71962   -11.7772056   3.606299   15.       ]\n",
            "83 [-26.85946   -13.76522   -16.5184056   1.104599   16.       ]\n",
            "9 [-25.35176   -11.80562   -19.5768056   0.982169   17.       ]\n",
            "7 [-28.94336   -18.03412    -9.3379056  -0.172131   18.       ]\n",
            "32 [-27.59156   -16.97462   -11.6816056   0.227849   19.       ]\n",
            "57 [-27.29636   -12.08902   -16.8306056  -6.004451   20.       ]\n",
            "8 [-26.50946   -21.65532   -13.0439056   1.498949   19.       ]\n",
            "12 [-26.23615   -16.77802   -17.9633056  -4.320851   20.       ]\n",
            "19 [-24.21845   -14.97982   -20.9214056  -4.110951   21.       ]\n",
            "7 [-27.81695   -28.63912    -3.3162056  -6.603651   22.       ]\n",
            "7 [-26.23595   -27.77003    -5.6300056  -5.779531   23.       ]\n",
            "2 [-27.52055   -24.49853    -7.3971056  -9.040331   24.       ]\n",
            "19 [-28.09115   -24.47373    -8.6392056  -8.478231   23.       ]\n",
            "1 [-26.83395   -19.60063   -13.9253056 -14.352331   24.       ]\n",
            "4 [-24.80295   -17.74863   -16.9374056 -14.349328   25.       ]\n",
            "13 [-22.15505   -27.88603   -15.6064056  -8.878628   24.       ]\n",
            "102 [-19.96075   -23.33573   -20.5824056 -11.604028   25.       ]\n",
            "15 [-20.48025   -20.07243   -23.6719056 -10.619128   24.       ]\n",
            "362 [-22.04235   -22.28453   -19.4128056 -10.339408   25.       ]\n",
            "74 [-20.01135   -20.43253   -22.4249056 -10.336405   26.       ]\n",
            "14 [-23.09795   -27.06873   -11.8844056 -11.228225   27.       ]\n",
            "15 [-26.19135   -24.15103   -14.1076056 -11.451055   26.       ]\n",
            "42 [-27.63925   -19.27163   -22.4504056  -9.342455   25.       ]\n",
            "5 [-26.85235   -28.83793   -18.6637056  -1.839055   24.       ]\n",
            "6 [-25.27135   -27.96884   -20.9775056  -1.014935   25.       ]\n",
            "29 [-25.79082   -24.70554   -24.0670056  -0.030015   24.       ]\n",
            "36 [-26.12002   -20.25034   -28.6388056   0.958785   23.       ]\n",
            "6 [-32.54472   -10.71924   -28.6159616  -5.892915   24.       ]\n",
            "23 [-34.10192   -20.60004   -20.5071616  -6.973515   25.       ]\n",
            "7 [-34.62142   -17.33674   -23.5966616  -5.988615   24.       ]\n",
            "22 [-33.83452   -26.90304   -19.8099616   1.514785   23.       ]\n",
            "14 [-32.93045   -23.53224   -24.3086616  -2.181715   24.       ]\n",
            "30 [-30.91275   -21.73404   -27.2667616  -1.971815   25.       ]\n",
            "2 [-32.30145   -26.61134   -20.7893616  -1.630025   26.       ]\n",
            "8 [-32.14037   -20.14894   -29.1466616  -0.108425   25.       ]\n",
            "48 [-32.83916   -23.52604   -25.0255616   1.395875   26.       ]\n",
            "6 [-33.35866   -20.26274   -28.1150616   2.380775   25.       ]\n",
            "4 [-37.56966   -32.73634   -13.1446616   0.992375   26.       ]\n",
            "35 [-43.45146   -25.07794   -12.5888616  -1.923125   27.       ]\n",
            "1 [-43.87111   -22.16854   -14.3747616  -4.130025   28.       ]\n",
            "12 [-42.97599   -17.39474   -19.2178616  -9.720925   29.       ]\n",
            "3 [-43.49549   -14.13144   -22.3073616  -8.736025   28.       ]\n",
            "8 [-40.84759   -24.26884   -20.9763616  -3.265325   27.       ]\n",
            "137 [-38.65329   -19.71854   -25.9523616  -5.990725   28.       ]\n",
            "16 [-36.45899   -15.16824   -30.9283616  -8.716125   29.       ]\n",
            "28 [-36.33275   -25.48984   -27.2162616  -2.597625   28.       ]\n",
            "3 [-36.66199   -21.03464   -31.7880616  -1.608825   27.       ]\n",
            "17 [-40.67929   -29.34694   -19.3333616  -3.046325   28.       ]\n",
            "18 [-39.08889   -27.13484   -22.4516616  -3.163575   29.       ]\n",
            "66 [-39.60839   -23.87154   -25.5411616  -2.178675   28.       ]\n",
            "22 [-39.93759   -19.41634   -30.1129616  -1.189875   27.       ]\n",
            "23 [-41.87849   -28.10114   -20.9579616  -0.249385   28.       ]\n",
            "33 [-40.14539   -24.14674   -25.6991616  -2.751085   29.       ]\n",
            "86 [-40.66489   -20.88344   -28.7886616  -1.766185   28.       ]\n",
            "1 [-38.27319   -16.32694   -33.7774616  -4.664885   29.       ]\n",
            "66 [-39.20906   -21.42774   -29.2407616  -3.278285   30.       ]\n",
            "26 [-37.17806   -19.57574   -32.2528616  -3.275282   31.       ]\n",
            "9 [-40.77656   -33.23504   -14.6476616  -5.767982   32.       ]\n",
            "40 [-40.16925   -29.28064   -19.4196616 -10.253282   33.       ]\n",
            "207 [-40.87495   -23.78254   -27.7564616  -7.381782   32.       ]\n",
            "1 [-41.39445   -20.51924   -30.8459616  -6.396882   31.       ]\n",
            "4 [-44.34425   -28.79224   -20.5813616  -5.233982   32.       ]\n",
            "141 [-42.66435   -24.58544   -25.1211616  -7.627082   33.       ]\n",
            "34 [-40.64665   -22.78724   -28.0792616  -7.417182   34.       ]\n",
            "17 [-41.16612   -19.52394   -31.1687616  -6.432262   33.       ]\n",
            "221 [-41.8262    -22.74994   -27.3629616  -5.248662   34.       ]\n",
            "8 [-39.6319    -18.19964   -32.3389616  -7.974062   35.       ]\n",
            "70 [-38.3319    -28.46744   -29.3859616  -2.110262   34.       ]\n",
            "10 [-35.9402    -23.91094   -34.3747616  -5.008962   35.       ]\n",
            "22 [-38.7793    -30.54094   -23.8898616  -5.430092   36.       ]\n",
            "113 [-37.0462    -26.58654   -28.6310616  -7.931792   37.       ]\n",
            "37 [-37.3754    -22.13134   -33.2028616  -6.942992   36.       ]\n",
            "8 [-38.7722    -31.80114   -23.7376616  -7.291712   37.       ]\n",
            "32 [-39.10144   -27.34594   -28.3094616  -6.302912   36.       ]\n",
            "57 [-39.62094   -24.08264   -31.3989616  -5.318012   35.       ]\n",
            "3 [-43.37124   -37.54124   -13.8057616  -8.095112   36.       ]\n",
            "7 [-43.89074   -34.27794   -16.8952616  -7.110212   35.       ]\n",
            "33 [-43.28343   -30.32354   -21.6672616 -11.595512   36.       ]\n",
            "39 [-41.83333   -26.71684   -25.7229616 -13.192112   37.       ]\n",
            "109 [-39.80233   -24.86484   -28.7350616 -13.189109   38.       ]\n",
            "12 [-40.32183   -21.60154   -31.8245616 -12.204209   37.       ]\n",
            "7 [-38.46343   -29.48754   -30.1602616 -10.365809   36.       ]\n",
            "121 [-36.44573   -27.68934   -33.1183616 -10.155909   37.       ]\n",
            "22 [-36.96523   -24.42604   -36.2078616  -9.171009   36.       ]\n",
            "45 [-41.25393   -32.28934   -24.3691616 -11.068809   37.       ]\n",
            "8 [-40.60921   -27.68314   -32.7161616  -8.358909   36.       ]\n",
            "34 [-38.59151   -25.88494   -35.6742616  -8.149009   37.       ]\n",
            "9 [-43.63921   -31.68724   -24.4302616  -8.539109   38.       ]\n",
            "29 [-42.18911   -28.08054   -28.4859616 -10.135709   39.       ]\n",
            "12 [-39.99481   -23.53024   -33.4619616 -12.861109   40.       ]\n",
            "13 [-43.26401   -36.27084   -17.9046616 -13.002929   41.       ]\n",
            "5 [-43.10293   -29.80844   -26.2619616 -11.481329   40.       ]\n",
            "20 [-41.52193   -28.93935   -28.5757616 -10.657209   41.       ]\n",
            "55 [-41.85113   -24.48415   -33.1475616  -9.668409   40.       ]\n",
            "63 [-45.08163   -31.69765   -21.5042616 -10.614539   41.       ]\n",
            "8 [-43.05063   -29.84565   -24.5163616 -10.611536   42.       ]\n",
            "175 [-41.37073   -25.63885   -29.0561616 -13.004636   43.       ]\n",
            "65 [-38.72283   -35.77625   -27.7251616  -7.533936   42.       ]\n",
            "39 [-39.24233   -32.51295   -30.8146616  -6.549036   41.       ]\n",
            "73 [-39.7618    -29.24965   -33.9041616  -5.564116   40.       ]\n",
            "35 [-40.09104   -24.79445   -38.4759616  -4.575316   39.       ]\n",
            "21 [-42.09764   -31.51345   -29.4597616  -4.475331   40.       ]\n",
            "118 [-42.61714   -28.25015   -32.5492616  -3.490431   39.       ]\n",
            "77 [-43.13664   -24.98685   -35.6387616  -2.505531   38.       ]\n",
            "13 [-46.73784   -31.52575   -25.1153616  -2.995201   39.       ]\n",
            "93 [-47.06708   -27.07055   -29.6871616  -2.006401   38.       ]\n",
            "91 [-45.03608   -25.21855   -32.6992616  -2.003398   39.       ]\n",
            "15 [-47.87518   -31.84855   -22.2143616  -2.424528   40.       ]\n",
            "50 [-46.61798   -26.97545   -27.5004616  -8.298628   41.       ]\n",
            "6 [-44.42368   -22.42515   -32.4764616 -11.024028   42.       ]\n",
            "15 [-44.94318   -19.16185   -35.5659616 -10.039128   41.       ]\n",
            "124 [-44.995159  -26.21395   -33.5118616  -6.888328   40.       ]\n",
            "2 [-48.691259  -39.89185   -15.9323616  -9.506428   41.       ]\n",
            "12 [-47.201659  -36.46305   -19.9632616 -10.932328   42.       ]\n",
            "86 [-49.206759  -29.59925   -28.0952616 -10.692228   41.       ]\n",
            "164 [-47.473659  -25.64485   -32.8364616 -13.193928   42.       ]\n",
            "34 [-45.615259  -33.53085   -31.1721616 -11.355528   41.       ]\n",
            "11 [-45.944459  -29.07565   -35.7439616 -10.366728   40.       ]\n",
            "22 [-46.463929  -25.81235   -38.8334616  -9.381808   39.       ]\n",
            "55 [-49.606229  -38.84885   -23.1561616 -10.043458   40.       ]\n",
            "17 [-47.575229  -36.99685   -26.1682616 -10.040455   41.       ]\n",
            "97 [-46.318029  -32.12375   -31.4543616 -15.914555   42.       ]\n",
            "128 [-44.727629  -29.91165   -34.5726616 -16.031805   43.       ]\n",
            "3 [-43.940729  -39.47795   -30.7859616  -8.528405   42.       ]\n",
            "127 [-41.549029  -34.92145   -35.7747616 -11.427105   43.       ]\n",
            "5 [-42.068529  -31.65815   -38.8642616 -10.442205   42.       ]\n",
            "75 [-43.457229  -36.53545   -32.3868616 -10.100415   43.       ]\n",
            "100 [-43.786429  -32.08025   -36.9586616  -9.111615   42.       ]\n",
            "4 [-47.016929  -39.29375   -25.3153616 -10.057745   43.       ]\n",
            "72 [-49.178529  -32.41335   -33.4670616  -9.976697   42.       ]\n",
            "11 [-49.507769  -27.95815   -38.0388616  -8.987897   41.       ]\n",
            "19 [-53.106269  -41.61745   -20.4336616 -11.480597   42.       ]\n",
            "14 [-52.365599  -39.88755   -23.6299616 -11.626297   43.       ]\n",
            "22 [-52.885099  -36.62425   -26.7194616 -10.641397   42.       ]\n",
            "15 [-51.434999  -33.01755   -30.7751616 -12.237997   43.       ]\n",
            "101 [-51.954499  -29.75425   -33.8646616 -11.253097   42.       ]\n",
            "9 [-52.473969  -26.49095   -36.9541616 -10.268177   41.       ]\n",
            "1 [-56.170069  -40.16885   -19.3746616 -12.886277   42.       ]\n",
            "34 [-54.490169  -35.96205   -23.9144616 -15.279377   43.       ]\n",
            "25 [-52.899769  -33.74995   -27.0327616 -15.396627   44.       ]\n",
            "46 [-53.605469  -28.25185   -35.3695616 -12.525127   43.       ]\n",
            "65 [-54.124939  -24.98855   -38.4590616 -11.540207   42.       ]\n",
            "22 [-56.662239  -31.94755   -29.6536616 -10.011307   43.       ]\n",
            "15 [-54.982339  -27.74075   -34.1934616 -12.404407   44.       ]\n",
            "7 [-55.501839  -24.47745   -37.2829616 -11.419507   43.       ]\n",
            "32 [-54.201839  -34.74525   -34.3299616  -5.555707   42.       ]\n",
            "91 [-52.694139  -32.78565   -37.3883616  -5.678137   43.       ]\n",
            "16 [-53.023339  -28.33045   -41.9601616  -4.689337   42.       ]\n",
            "3 [-57.045139  -36.63445   -29.4051616  -6.199237   43.       ]\n",
            "80 [-54.653439  -32.07795   -34.3939616  -9.097937   44.       ]\n",
            "14 [-52.865939  -27.29795   -39.5301616 -12.334137   45.       ]\n",
            "245 [-52.079039  -36.86425   -35.7434616  -4.830737   44.       ]\n",
            "4 [-50.048039  -35.01225   -38.7555616  -4.827734   45.       ]\n",
            "112 [-48.030339  -33.21405   -41.7136616  -4.617834   46.       ]\n",
            "61 [-48.549839  -29.95075   -44.8031616  -3.632934   45.       ]\n",
            "2 [-52.937439  -37.67745   -32.8376616  -5.087234   46.       ]\n",
            "82 [-53.456939  -34.41415   -35.9271616  -4.102334   45.       ]\n",
            "52 [-51.967339  -30.98535   -39.9580616  -5.528234   46.       ]\n",
            "20 [-52.486839  -27.72205   -43.0475616  -4.543334   45.       ]\n",
            "8 [-54.154539  -34.87555   -35.1546616  -3.575684   46.       ]\n",
            "26 [-51.960239  -30.32525   -40.1306616  -6.301084   47.       ]\n",
            "14 [-55.229439  -43.06585   -24.5733616  -6.442904   48.       ]\n",
            "56 [-53.198439  -41.21385   -27.5854616  -6.439901   49.       ]\n",
            "12 [-51.978639  -39.11565   -30.7808616  -6.311471   50.       ]\n",
            "75 [-50.770639  -35.04125   -35.5443616  -8.924371   51.       ]\n",
            "2 [-51.099879  -30.58605   -40.1161616  -7.935571   50.       ]\n",
            "10 [-54.795979  -44.26395   -22.5366616 -10.553671   51.       ]\n",
            "6 [-53.288279  -42.30435   -25.5950616 -10.676101   52.       ]\n",
            "36 [-55.380479  -35.49435   -34.0586616 -10.073941   51.       ]\n",
            "66 [-53.700579  -31.28755   -38.5984616 -12.467041   52.       ]\n",
            "10 [-54.220079  -28.02425   -41.6879616 -11.482141   51.       ]\n",
            "7 [-58.241879  -36.32825   -29.1329616 -12.992041   52.       ]\n",
            "78 [-56.651479  -34.11615   -32.2512616 -13.109291   53.       ]\n",
            "45 [-56.490399  -27.65375   -40.6085616 -11.587691   52.       ]\n",
            "28 [-59.996399  -40.22045   -25.4479616 -12.339851   53.       ]\n",
            "4 [-57.978699  -38.42225   -28.4060616 -12.129951   54.       ]\n",
            "119 [-55.586999  -33.86575   -33.3948616 -15.028651   55.       ]\n",
            "53 [-54.800099  -43.43205   -29.6081616  -7.525251   54.       ]\n",
            "16 [-53.349999  -39.82535   -33.6638616  -9.121851   55.       ]\n",
            "69 [-54.279699  -36.02825   -38.3067616  -8.826151   54.       ]\n",
            "4 [-51.887999  -31.47175   -43.2955616 -11.724851   55.       ]\n",
            "11 [-56.365499  -44.50205   -26.2121616 -14.759351   56.       ]\n",
            "3 [-57.295199  -40.70495   -30.8550616 -14.463651   55.       ]\n",
            "8 [-55.264199  -38.85295   -33.8671616 -14.460648   56.       ]\n",
            "48 [-55.593399  -34.39775   -38.4389616 -13.471848   55.       ]\n",
            "21 [-55.922639  -29.94255   -43.0107616 -12.483048   54.       ]\n",
            "196 [-53.274739  -40.07995   -41.6797616  -7.012348   53.       ]\n",
            "6 [-51.080439  -35.52965   -46.6557616  -9.737748   54.       ]\n",
            "4 [-54.586439  -48.09635   -31.4951616 -10.489908   55.       ]\n",
            "3 [-55.105939  -44.83305   -34.5846616  -9.505008   54.       ]\n",
            "112 [-53.598239  -42.87345   -37.6430616  -9.627438   55.       ]\n",
            "95 [-54.117739  -39.61015   -40.7325616  -8.642538   54.       ]\n",
            "56 [-54.637239  -36.34685   -43.8220616  -7.657638   53.       ]\n",
            "23 [-55.156709  -33.08355   -46.9115616  -6.672718   52.       ]\n",
            "52 [-59.889809  -39.26245   -35.5235616  -7.746818   53.       ]\n",
            "22 [-57.872109  -37.46425   -38.4816616  -7.536918   54.       ]\n",
            "13 [-56.192209  -33.25745   -43.0214616  -9.930018   55.       ]\n",
            "34 [-55.405309  -42.82375   -39.2347616  -2.426618   54.       ]\n",
            "5 [-53.617809  -38.04375   -44.3709616  -5.662818   55.       ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check the average perceptron accuracy."
      ],
      "metadata": {
        "id": "nc7JLE0xx6y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_average_perceptron(y_test: np.ndarray, x_test: np.ndarray, weights: np.ndarray):\n",
        "  num_observations: int = len(y_test)\n",
        "  correct: int = 0\n",
        "  for i in range(num_observations):\n",
        "    prediction = np.sign(np.dot(weights, x_test[i]))\n",
        "    if prediction == y_test[i]:\n",
        "      correct += 1\n",
        "  return correct / num_observations"
      ],
      "metadata": {
        "id": "A1yocdcWyEWG"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = Perceptron().average_perceptron(X = Perceptron.bias_trick(train.drop('label', axis=1).values), Y = train['label'].values, epoch=10)\n",
        "print(\"average perceptron weights: \", weights)\n",
        "print(\"train accuracy: \", accuracy_average_perceptron(train['label'].values, Perceptron.bias_trick(train.drop('label', axis=1).values), weights))\n",
        "print(\"test accuracy: \", accuracy_average_perceptron(test['label'].values, Perceptron.bias_trick(test.drop('label', axis=1).values), weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdnjrkr5yI1o",
        "outputId": "2a4dbd06-8cd7-4691-ed17-6babf6378c0c"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average perceptron weights:  [-432008.214314   -268086.78152299 -278974.1836      -74294.680935\n",
            "  359407.        ]\n",
            "train accuracy:  0.9850917431192661\n",
            "test accuracy:  0.986\n"
          ]
        }
      ]
    }
  ]
}